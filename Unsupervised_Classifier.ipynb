{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ffdab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "from re import sub\n",
    "from time import time \n",
    "from unidecode import unidecode\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "import glob\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520d7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_unsup=r'C:\\Users\\raja4\\Downloads\\aclImdb_v1.tar\\aclImdb\\train\\unsup'\n",
    "file_list_unsup = glob.glob(path_train_unsup + \"/*.txt\")\n",
    "unsup_reviews=[]\n",
    "for i in range(0,len(file_list_unsup)):\n",
    "    data=pd.read_table(file_list_unsup[i])\n",
    "    unsup_reviews.append(data.columns.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91883500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(unsup_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f6beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'0':'reviews'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28fc79df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, admit, the, great, majority, of, films, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[take, low, budget, inexperienced, actors, dou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[everybody, has, seen, back, to, the, future, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[doris, day, was, an, icon, of, beauty, in, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[after, series, of, silly, fun, loving, movies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>[delightfully, awful, made, by, david, giancol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>[watching, time, chasers, it, obvious, that, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>[at, the, beginning, we, can, see, members, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>[the, movie, was, incredible, ever, since, saw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>[tcm, came, through, by, acquiring, this, wond...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0      [i, admit, the, great, majority, of, films, re...\n",
       "1      [take, low, budget, inexperienced, actors, dou...\n",
       "2      [everybody, has, seen, back, to, the, future, ...\n",
       "3      [doris, day, was, an, icon, of, beauty, in, si...\n",
       "4      [after, series, of, silly, fun, loving, movies...\n",
       "...                                                  ...\n",
       "49995  [delightfully, awful, made, by, david, giancol...\n",
       "49996  [watching, time, chasers, it, obvious, that, i...\n",
       "49997  [at, the, beginning, we, can, see, members, of...\n",
       "49998  [the, movie, was, incredible, ever, since, saw...\n",
       "49999  [tcm, came, through, by, acquiring, this, wond...\n",
       "\n",
       "[50000 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff0aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    \"\"\"\n",
    "    Preprocess and convert texts to a list of words \n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove html tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Single character removal\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    text = text.split()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01af8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0] = df[0].apply(lambda x: text_to_word_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a52230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'admit',\n",
       " 'the',\n",
       " 'great',\n",
       " 'majority',\n",
       " 'of',\n",
       " 'films',\n",
       " 'released',\n",
       " 'before',\n",
       " 'say',\n",
       " 'are',\n",
       " 'just',\n",
       " 'not',\n",
       " 'for',\n",
       " 'me',\n",
       " 'of',\n",
       " 'the',\n",
       " 'dozen',\n",
       " 'or',\n",
       " 'so',\n",
       " 'major',\n",
       " 'silents',\n",
       " 'have',\n",
       " 'viewed',\n",
       " 'one',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'crowd',\n",
       " 'and',\n",
       " 'two',\n",
       " 'were',\n",
       " 'very',\n",
       " 'good',\n",
       " 'the',\n",
       " 'last',\n",
       " 'command',\n",
       " 'and',\n",
       " 'city_lights',\n",
       " 'that',\n",
       " 'latter',\n",
       " 'chaplin',\n",
       " 'circa',\n",
       " 'so',\n",
       " 'was',\n",
       " 'apprehensive_about',\n",
       " 'this',\n",
       " 'one',\n",
       " 'and',\n",
       " 'humor',\n",
       " 'is',\n",
       " 'often',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'appreciate',\n",
       " 'uh',\n",
       " 'enjoy',\n",
       " 'decades_later',\n",
       " 'did',\n",
       " 'like',\n",
       " 'the',\n",
       " 'lead',\n",
       " 'actors',\n",
       " 'but',\n",
       " 'thought',\n",
       " 'little',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " 'one',\n",
       " 'intriguing',\n",
       " 'sequence',\n",
       " 'early',\n",
       " 'on',\n",
       " 'the',\n",
       " 'guys',\n",
       " 'are',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'get',\n",
       " 'de',\n",
       " 'loused',\n",
       " 'and',\n",
       " 'for',\n",
       " 'about',\n",
       " 'three',\n",
       " 'minutes',\n",
       " 'fully_dressed',\n",
       " 'do',\n",
       " 'some',\n",
       " 'schtick',\n",
       " 'in',\n",
       " 'the',\n",
       " 'background',\n",
       " 'perhaps',\n",
       " 'three',\n",
       " 'dozen',\n",
       " 'men',\n",
       " 'pass',\n",
       " 'by',\n",
       " 'all',\n",
       " 'naked',\n",
       " 'white',\n",
       " 'and',\n",
       " 'black',\n",
       " 'wwi',\n",
       " 'and',\n",
       " 'for',\n",
       " 'most',\n",
       " 'their_butts',\n",
       " 'part',\n",
       " 'or',\n",
       " 'full',\n",
       " 'backside',\n",
       " 'are',\n",
       " 'shown',\n",
       " 'was',\n",
       " 'this',\n",
       " 'an',\n",
       " 'early',\n",
       " 'variation',\n",
       " 'of',\n",
       " 'beefcake',\n",
       " 'courtesy',\n",
       " 'of',\n",
       " 'howard_hughes']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparation pour Word2vec\n",
    "text = [row for row in df[0]]\n",
    "phrases = Phrases(text, min_count=1)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[text]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475d8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd6a5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.44 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     vector_size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "282250ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 12.68 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42527c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f286525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189818"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d2a1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting preprocessed dataset for further steps with replaced bigrams\n",
    "file_export = df.copy()\n",
    "file_export[0] = file_export[0].apply(lambda x: ' '.join(bigram[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50f0a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f41badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export[0].to_csv(\"cleaned_review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bc2e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d8bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors.astype('double'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e80163e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.23821567e-02,  2.87262650e-02,  8.04570083e-02,\n",
       "        -2.16880511e-01,  3.02361751e-02, -7.31453005e-02,\n",
       "         8.05857639e-03,  1.97015421e-01, -1.35185409e-01,\n",
       "        -1.30321115e-01,  3.12745553e-02, -1.34941779e-01,\n",
       "         5.01728609e-02,  5.71345045e-02, -8.39376482e-02,\n",
       "        -2.25920397e-01,  1.50311380e-01,  1.05084617e-01,\n",
       "        -1.94330481e-02,  1.18265210e-01, -7.02730020e-02,\n",
       "         1.75064757e-01, -6.40603506e-02,  6.04359350e-02,\n",
       "         3.37815531e-02, -2.45680053e-02, -8.25709331e-02,\n",
       "        -2.30156535e-02,  4.90064924e-02, -6.92149633e-03,\n",
       "         1.12402394e-01, -2.77165736e-02, -3.06819771e-02,\n",
       "         4.63786665e-02, -1.87095047e-02, -5.48662081e-02,\n",
       "         1.44344680e-01, -7.33715005e-02, -4.25305729e-02,\n",
       "         8.60211206e-02,  1.46294880e-03, -6.28088384e-02,\n",
       "         1.39743029e-01,  3.53359788e-02,  2.77570057e-02,\n",
       "         3.56498224e-02,  4.59476117e-02,  1.66229538e-02,\n",
       "         5.15331307e-02,  8.94209402e-02, -9.59511837e-02,\n",
       "        -1.67695191e-02, -1.18674451e-02,  3.09344787e-02,\n",
       "         1.51935443e-02, -2.01820536e-03, -4.60232466e-03,\n",
       "         7.10124667e-02,  3.25064949e-02,  6.72494440e-02,\n",
       "        -1.97688788e-02,  1.61166740e-01, -2.45406081e-02,\n",
       "        -1.68030729e-02, -7.69029135e-03, -4.98421938e-02,\n",
       "         8.12111829e-02,  8.00154557e-03,  3.26459134e-03,\n",
       "        -3.46279949e-02, -2.40245067e-02,  9.01936125e-02,\n",
       "         1.76044861e-01, -3.54518024e-02,  3.81445813e-02,\n",
       "         2.02622737e-01, -4.33041015e-02, -4.53227843e-02,\n",
       "        -9.60813199e-02,  1.80829086e-01,  3.26917692e-03,\n",
       "         1.94843334e-02, -3.72082660e-02,  1.67674114e-01,\n",
       "         2.57514945e-02, -8.60329580e-03, -8.39978441e-02,\n",
       "        -3.35232960e-02,  6.24821308e-02,  1.65433108e-01,\n",
       "        -1.22935545e-02,  2.83043070e-02, -7.87651279e-02,\n",
       "         1.70053454e-01,  2.24829901e-01, -1.21812705e-02,\n",
       "         1.49287635e-01, -3.65163331e-02, -4.08559082e-03,\n",
       "         1.41842163e-01, -4.04230292e-02,  4.30094299e-02,\n",
       "        -1.11048088e-02, -1.11347417e-01, -8.58565385e-02,\n",
       "         7.01731666e-02, -5.08163952e-02, -2.58103480e-02,\n",
       "        -5.29334848e-02, -4.53642771e-02, -1.35958496e-01,\n",
       "         3.64197348e-03,  8.62995911e-02,  1.36745289e-01,\n",
       "         3.53109162e-02,  5.30076917e-02, -1.93210707e-02,\n",
       "         2.89399528e-04,  9.58007924e-02,  3.41205478e-02,\n",
       "         4.92670332e-02,  5.40447748e-02,  1.49083458e-02,\n",
       "        -3.52481440e-02, -8.31596803e-02, -1.37074007e-02,\n",
       "         1.40308781e-02, -6.69508733e-02,  2.95550439e-02,\n",
       "         1.03896018e-01, -5.01662441e-02,  1.94565457e-02,\n",
       "         1.75028194e-01, -6.49135498e-02,  8.57708189e-02,\n",
       "         1.58789364e-02, -9.31352630e-02,  9.44191126e-02,\n",
       "         4.27837525e-02, -2.21715287e-02, -1.32465685e-02,\n",
       "        -1.40053586e-01,  2.92237515e-02, -2.87702349e-02,\n",
       "         7.49119287e-02, -1.06558313e-01, -3.38723008e-02,\n",
       "        -1.40130201e-01,  7.91863350e-02,  9.96280309e-02,\n",
       "        -4.81054937e-02, -3.59141808e-02, -4.33388828e-02,\n",
       "         4.13368949e-02, -3.47108829e-02, -1.12677785e-01,\n",
       "        -3.82209700e-02,  4.71130849e-03, -1.02500028e-01,\n",
       "         8.34879744e-03,  4.06124133e-02, -7.36194407e-02,\n",
       "        -3.67826144e-02,  2.50841134e-01,  4.22189894e-02,\n",
       "         1.52345486e-01, -2.24946658e-03,  5.36613070e-03,\n",
       "        -4.27504414e-02, -5.18537962e-02, -3.57938940e-02,\n",
       "         6.84762211e-02,  1.78117695e-02, -1.59087234e-01,\n",
       "        -9.23877173e-03,  1.46866292e-01, -1.51221968e-01,\n",
       "         1.19733878e-01, -1.36472091e-02,  4.16511061e-02,\n",
       "        -1.94936199e-02,  1.95414510e-02,  5.52060654e-02,\n",
       "        -8.89276390e-02, -3.94925533e-02,  9.24983247e-02,\n",
       "         1.40679928e-01,  1.58369752e-01, -5.88359304e-02,\n",
       "         9.25362315e-03,  5.37857042e-02, -7.30574079e-02,\n",
       "        -9.71629189e-02,  5.68228318e-02, -1.37733946e-02,\n",
       "         5.16453120e-02, -7.92654473e-02, -8.45822563e-02,\n",
       "        -4.91031660e-02,  1.12473246e-01, -1.40560648e-01,\n",
       "         5.68470987e-02,  5.83885298e-02, -4.49203558e-02,\n",
       "        -1.76470535e-02, -4.42807771e-02, -2.26625999e-01,\n",
       "         6.02651297e-02, -9.09314647e-02, -1.16327147e-01,\n",
       "         2.56288663e-02, -4.35562197e-02,  1.77028543e-01,\n",
       "         1.31318274e-02,  4.29505286e-02, -2.14152143e-01,\n",
       "        -4.94416004e-03, -2.20167258e-03,  1.08035660e-01,\n",
       "        -1.84363941e-02, -1.14913880e-02,  5.10364006e-02,\n",
       "         1.26844856e-01,  2.08587331e-02,  9.26525909e-02,\n",
       "        -8.71149772e-02,  5.24769494e-02,  3.48982522e-02,\n",
       "        -4.47401107e-02,  6.45201414e-02,  4.17261728e-02,\n",
       "        -1.05919856e-01, -2.43524623e-02,  8.19100222e-02,\n",
       "        -5.74519957e-02, -1.36487449e-01, -7.35199955e-02,\n",
       "        -7.15107936e-02,  1.89149935e-01,  1.57677743e-04,\n",
       "        -8.32967669e-02, -1.41151547e-02, -1.46794841e-01,\n",
       "         6.75558503e-02, -2.41563444e-02, -6.53283873e-02,\n",
       "        -5.22309044e-02,  4.48657336e-03, -4.93920962e-02,\n",
       "        -1.19004915e-02, -4.61042557e-03,  1.40709485e-01,\n",
       "         7.61773392e-04, -6.50478123e-02, -8.77816978e-02,\n",
       "        -1.28231426e-01, -7.86029721e-02, -3.91454159e-02,\n",
       "        -4.94500881e-02,  1.95651784e-02,  9.65664127e-03,\n",
       "         5.81896043e-02, -8.69170700e-02,  5.01776461e-02,\n",
       "        -9.39010272e-02, -1.95094060e-02,  1.63499492e-02,\n",
       "         5.17829298e-02,  2.10417496e-02, -5.13419276e-02,\n",
       "         5.09146972e-03,  1.23147306e-01, -7.48105916e-02,\n",
       "        -3.54340491e-02,  7.28993791e-03, -1.25799578e-01,\n",
       "         1.23997838e-01,  4.12690502e-02, -3.34176958e-02,\n",
       "         7.62270709e-02,  7.76982472e-02,  2.55422390e-02,\n",
       "         2.34492266e-02, -9.13987024e-02, -1.60299391e-01,\n",
       "         5.76671545e-02,  2.85332762e-02,  1.61018083e-01,\n",
       "        -5.52947719e-02, -1.12220309e-01,  1.08160157e-01,\n",
       "        -1.17762128e-01,  9.63227142e-02,  1.11836302e-01,\n",
       "        -9.20307760e-03,  2.22540077e-01, -9.17364131e-02,\n",
       "        -5.63357945e-02,  1.97949116e-02,  2.97994284e-02],\n",
       "       [-7.41790385e-02, -1.47132893e-01,  7.32643029e-02,\n",
       "        -9.27265686e-02,  1.04303619e-01,  6.70509389e-02,\n",
       "        -1.01266366e-02,  1.17584636e-04, -6.59081420e-03,\n",
       "         3.56338684e-03,  1.03418974e-01, -1.21091893e-01,\n",
       "         1.54307384e-01,  8.28193076e-02, -9.82087026e-02,\n",
       "        -1.42318531e-01,  4.82902015e-03,  1.26041211e-02,\n",
       "         1.60395543e-01,  1.95008083e-01, -1.15930584e-01,\n",
       "         1.05591176e-01, -1.57415847e-01,  2.88642422e-04,\n",
       "         8.54060601e-02,  1.02880407e-04,  1.67546833e-02,\n",
       "        -4.91763640e-02,  6.40448520e-02,  8.99222540e-02,\n",
       "        -8.57901215e-03,  2.64715664e-02, -7.08182748e-02,\n",
       "         7.26625945e-02, -6.79203871e-02, -1.54352884e-01,\n",
       "        -4.90696314e-02,  1.64410495e-01, -8.70457887e-02,\n",
       "         1.36553319e-02,  1.70092107e-02, -1.69966935e-02,\n",
       "         1.12072273e-01,  3.39370534e-02, -7.24448530e-02,\n",
       "        -9.02005852e-02,  1.79349167e-01,  1.31767887e-01,\n",
       "        -1.18748876e-01,  2.91062535e-04, -6.71246050e-02,\n",
       "        -2.29107584e-01, -1.57564108e-01, -4.07913938e-02,\n",
       "         1.22260696e-02, -6.30177198e-03,  8.85250594e-02,\n",
       "         8.25387813e-02,  1.30760781e-01,  7.76887981e-02,\n",
       "         5.68493749e-02,  2.37035468e-01,  6.73936267e-02,\n",
       "        -6.58377850e-02, -7.00121360e-02, -7.80615142e-02,\n",
       "         6.50687646e-02, -4.63064816e-02, -9.08453118e-03,\n",
       "        -1.59602071e-01, -1.76714919e-01,  3.29811100e-03,\n",
       "        -3.12257100e-02,  1.24772247e-01, -3.94645819e-02,\n",
       "        -1.47215523e-02,  8.48479572e-02,  3.85936494e-02,\n",
       "        -1.25372100e-01,  9.41203779e-02,  1.76315508e-01,\n",
       "        -1.07894498e-01,  1.64538921e-03,  6.53405888e-02,\n",
       "         2.18685328e-02,  2.24641843e-02, -1.27824532e-01,\n",
       "         6.05143146e-02,  8.43476905e-02, -7.78514310e-02,\n",
       "        -8.06625535e-02,  5.39673042e-02, -4.06050300e-02,\n",
       "         1.90364276e-01,  2.39036704e-01,  1.82123622e-02,\n",
       "         4.49784129e-02, -4.94476646e-03,  3.17297067e-02,\n",
       "         3.77414635e-02,  1.56816700e-01, -1.00321590e-01,\n",
       "         4.76068782e-02, -1.20292512e-01, -1.08050129e-01,\n",
       "         1.08684903e-01, -3.10768334e-01, -2.91853172e-02,\n",
       "        -4.66584620e-02, -3.84310287e-02,  1.67970359e-01,\n",
       "        -8.43173037e-02,  1.08578592e-01,  2.10605108e-02,\n",
       "        -4.49687452e-02,  7.38475405e-02,  8.84151680e-02,\n",
       "        -1.02193604e-01, -8.39751390e-02,  1.17904408e-01,\n",
       "        -7.03626584e-02, -1.51515495e-01, -2.21014063e-02,\n",
       "        -6.07007673e-02, -1.81354309e-02,  3.62487742e-02,\n",
       "         9.72075369e-02,  5.54764817e-02, -6.95033355e-02,\n",
       "         1.23552168e-01, -6.92056315e-02,  1.78332374e-02,\n",
       "         5.04273807e-02, -9.28860993e-02,  1.24890647e-01,\n",
       "        -8.79136230e-02, -2.93643738e-02,  7.48585934e-02,\n",
       "         3.73977417e-02,  3.33459478e-02,  4.41013475e-02,\n",
       "         1.77610298e-02,  4.03834338e-02, -1.14706099e-01,\n",
       "         2.41462676e-01, -7.64829362e-02, -7.90754607e-03,\n",
       "         2.17068300e-01, -5.34349127e-02,  6.98260465e-02,\n",
       "        -1.72202225e-02,  1.09290488e-01, -1.58290821e-02,\n",
       "         1.97181551e-03, -2.27505151e-02, -1.03468622e-01,\n",
       "        -7.21832635e-02, -1.00742423e-01, -8.01440034e-02,\n",
       "        -4.03848069e-02, -7.95650397e-02,  3.40720687e-03,\n",
       "         3.84194044e-02,  1.73159038e-01, -2.77063052e-02,\n",
       "         4.78111750e-02, -7.38544776e-02,  9.14138934e-05,\n",
       "         1.52075638e-02, -1.32943578e-02,  1.81850955e-01,\n",
       "         1.23960272e-02,  2.39246867e-02, -8.13507923e-02,\n",
       "        -1.46259935e-01,  8.15232287e-02,  4.39461301e-02,\n",
       "         8.27693256e-02, -1.01003360e-01,  1.46199359e-02,\n",
       "         8.06230022e-02, -9.62401914e-02,  6.97831636e-02,\n",
       "        -1.83651901e-01,  9.65637063e-02, -1.73920094e-02,\n",
       "         5.86020604e-02, -4.29710193e-02, -1.33458544e-01,\n",
       "         1.46347272e-01,  2.92692843e-02, -9.96831224e-02,\n",
       "         5.56675782e-02,  6.63092187e-02,  2.22182921e-02,\n",
       "         1.09868223e-01,  7.25637964e-02,  3.88793878e-02,\n",
       "         1.49078580e-02,  6.87020518e-02, -5.16991814e-02,\n",
       "         1.17938105e-01,  5.64205390e-02,  6.23223461e-02,\n",
       "        -1.82586781e-01,  8.10353301e-02, -1.23577061e-01,\n",
       "        -4.49147344e-02,  1.37297370e-03, -9.42677954e-02,\n",
       "         3.11127381e-04,  1.54024976e-01, -9.12847554e-02,\n",
       "        -2.46755888e-02, -4.06791975e-02, -1.30909642e-01,\n",
       "         1.15201975e-01,  2.20175410e-01,  1.84123090e-01,\n",
       "        -9.38455548e-03, -1.52542215e-01,  3.95776249e-02,\n",
       "         1.65911138e-01,  1.10259759e-01,  1.05450121e-01,\n",
       "        -9.06276339e-02, -1.13698475e-01,  4.15260296e-02,\n",
       "         6.13147418e-03, -7.54569912e-02, -1.40257532e-01,\n",
       "        -8.42456286e-02, -8.00731853e-03,  1.16154968e-01,\n",
       "         8.51581589e-03, -1.14092669e-01, -6.50304010e-02,\n",
       "         1.26136045e-01,  1.16775426e-01,  1.56903714e-01,\n",
       "        -2.97446008e-02,  1.50369892e-02, -7.40069513e-02,\n",
       "         3.85304386e-02, -9.05461747e-03, -1.93463894e-01,\n",
       "        -5.52957651e-02,  1.38684188e-01, -1.70951189e-01,\n",
       "         5.55243828e-03, -4.23271310e-02,  4.89071430e-02,\n",
       "        -4.88985705e-02, -2.70800460e-02,  6.14247197e-02,\n",
       "        -8.67254627e-02, -1.64844799e-01, -2.63214951e-02,\n",
       "         1.70203696e-01,  2.58760503e-02,  5.40548190e-02,\n",
       "         2.37970543e-01, -1.60636238e-01, -2.98001904e-02,\n",
       "         6.68242796e-02,  2.25378439e-02,  7.32148818e-02,\n",
       "         4.52531869e-02,  1.44691658e-01, -4.72904344e-02,\n",
       "         6.50592141e-02,  3.60855480e-02, -7.39589917e-02,\n",
       "        -1.16418380e-01, -1.09213241e-01, -2.30968913e-01,\n",
       "         2.03829068e-01,  2.49351688e-01,  1.45418553e-01,\n",
       "         5.07094055e-02,  1.11533592e-01,  2.93582169e-02,\n",
       "         4.49535710e-02, -2.60274828e-01, -1.61822442e-01,\n",
       "         1.54391487e-01,  2.26306672e-02,  7.40616173e-02,\n",
       "        -9.30108390e-02, -2.81307895e-01,  2.38063609e-02,\n",
       "        -1.97639115e-01,  2.41260360e-02,  1.27457853e-01,\n",
       "        -2.30431003e-01,  1.77756211e-01, -1.08724552e-01,\n",
       "        -1.58484408e-01, -8.51174998e-03,  3.49035241e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5676ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('st_ds', 0.8356879353523254),\n",
       " ('sceens', 0.8340104818344116),\n",
       " ('worth_skipping', 0.8250850439071655),\n",
       " ('obvious_nods', 0.819431483745575),\n",
       " ('speilberg', 0.8189619779586792),\n",
       " ('yessir', 0.8170914053916931),\n",
       " ('previous_commentators', 0.8150827288627625),\n",
       " ('wildly_imaginative', 0.814156711101532),\n",
       " ('netherbeast', 0.8128885626792908),\n",
       " ('inundated', 0.8107444047927856)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee6a807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_center = model.cluster_centers_[1]\n",
    "negative_cluster_center = model.cluster_centers_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "faf35333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delatour', 0.883544385433197),\n",
       " ('roberts_blossom', 0.8794320225715637),\n",
       " ('lindon', 0.8793892860412598),\n",
       " ('carole_mathews', 0.8779892921447754),\n",
       " ('kyra_helene', 0.8777850866317749),\n",
       " ('peewee', 0.8738564252853394),\n",
       " ('hammerson', 0.8733760714530945),\n",
       " ('dana_elcar', 0.873170018196106),\n",
       " ('jodies', 0.8722919225692749),\n",
       " ('horse_races', 0.8705471158027649)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90478de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_vectors.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34f0d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.columns = ['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "187a0067",
   "metadata": {},
   "outputs": [],
   "source": [
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3d4b48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 63.65 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "words[\"cluster\"] = words[\"vectors\"].apply(lambda x: model.predict([x.astype('double')]))\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53ac50c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"vec=[]\\n\\nfor i in range(len(words['vectors'])):\\n    vec.append(model.predict(words['vectors'][i].reshape(1, -1).astype('float')))\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"vec=[]\n",
    "\n",
    "for i in range(len(words['vectors'])):\n",
    "    vec.append(model.predict(words['vectors'][i].reshape(1, -1).astype('float')))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fc9e928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(vec)'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"len(vec)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5011929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>[-0.21185513, -0.15422396, 0.19057572, -0.1520...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>[-0.09514717, 0.018168183, -0.044136487, -0.39...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>[-0.30162582, -0.08878651, -0.124232866, -0.29...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>[-0.6773318, -0.26445127, 0.24340089, -0.26405...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>[-0.641791, -0.35387745, 0.124121964, -0.05242...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93235</th>\n",
       "      <td>paramount_classic</td>\n",
       "      <td>[0.16848078, 0.07179757, -0.026129965, -0.1165...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93236</th>\n",
       "      <td>ebert_review</td>\n",
       "      <td>[0.0346315, 0.14012042, 0.018588578, -0.038789...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93237</th>\n",
       "      <td>nurse_packer</td>\n",
       "      <td>[-0.09018444, -0.29930675, 0.11089235, -0.0565...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93238</th>\n",
       "      <td>neediness</td>\n",
       "      <td>[-0.058380682, -0.015228618, 0.041239202, -0.3...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93239</th>\n",
       "      <td>turbulent_period</td>\n",
       "      <td>[0.10967116, -0.14216748, 0.1168057, -0.138146...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   words                                            vectors  \\\n",
       "0                    the  [-0.21185513, -0.15422396, 0.19057572, -0.1520...   \n",
       "1                    and  [-0.09514717, 0.018168183, -0.044136487, -0.39...   \n",
       "2                     of  [-0.30162582, -0.08878651, -0.124232866, -0.29...   \n",
       "3                     to  [-0.6773318, -0.26445127, 0.24340089, -0.26405...   \n",
       "4                     is  [-0.641791, -0.35387745, 0.124121964, -0.05242...   \n",
       "...                  ...                                                ...   \n",
       "93235  paramount_classic  [0.16848078, 0.07179757, -0.026129965, -0.1165...   \n",
       "93236       ebert_review  [0.0346315, 0.14012042, 0.018588578, -0.038789...   \n",
       "93237       nurse_packer  [-0.09018444, -0.29930675, 0.11089235, -0.0565...   \n",
       "93238          neediness  [-0.058380682, -0.015228618, 0.041239202, -0.3...   \n",
       "93239   turbulent_period  [0.10967116, -0.14216748, 0.1168057, -0.138146...   \n",
       "\n",
       "      cluster  \n",
       "0         [0]  \n",
       "1         [0]  \n",
       "2         [0]  \n",
       "3         [0]  \n",
       "4         [0]  \n",
       "...       ...  \n",
       "93235     [0]  \n",
       "93236     [0]  \n",
       "93237     [1]  \n",
       "93238     [0]  \n",
       "93239     [0]  \n",
       "\n",
       "[93240 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f080b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.cluster = words.cluster.apply(lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dcefbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bb497f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "935ae8dc",
   "metadata": {},
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09c4d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12c29508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f3257de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = pd.read_csv(\"sentiment_dictionary.csv\")\n",
    "sentiment_dict = dict(zip(sentiment_map[\"words\"].values, sentiment_map[\"sentiment_coeff\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f0b06f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file_weighting = unsup_reviews.copy()\\nfile_weighting = pd.DataFrame(file_weighting)'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''file_weighting = unsup_reviews.copy()\n",
    "file_weighting = pd.DataFrame(file_weighting)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "956c2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_csv(\"cleaned_review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5101fa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raja4\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\raja4\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf.fit(review[\"0\"].values.astype('U'))\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "review_tfidf = tfidf.transform(review[\"0\"].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ccb0c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    '''\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "    \n",
    "    inspired  by function from this article: \n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    \n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    '''\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], str(x['0']).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a95d4f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        i admit the great majority of films released b...\n",
       "1        take low_budget inexperienced_actors doubling_...\n",
       "2        everybody has seen back to the future right wh...\n",
       "3        doris_day was an icon of beauty in singing and...\n",
       "4        after series of silly fun_loving movies was bi...\n",
       "                               ...                        \n",
       "49995    delightfully awful made by david_giancola guy ...\n",
       "49996    watching time_chasers it obvious that it was m...\n",
       "49997    at the beginning we can see members of troma_t...\n",
       "49998    the movie was incredible ever since saw it in ...\n",
       "49999    tcm came through by acquiring this wonderful s...\n",
       "Name: 0, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4923d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.8 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "replaced_tfidf_scores = review.apply(lambda x: replace_tfidf_words(x, review_tfidf, features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d93a6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    \"\"\"\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc41dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_closeness_scores = review[\"0\"].apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), str(x).split())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6b29e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, review[\"0\"]]).T\n",
    "replacement_df.columns = [\"sentiment_coeff\", \"tfidf_scores\", \"review\"]\n",
    "replacement_df[\"sentiment_rate\"] = replacement_df.apply(lambda x: np.array(x.loc[\"sentiment_coeff\"]) @ np.array(x.loc[\"tfidf_scores\"]), axis=1)\n",
    "replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25bc9134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_coeff</th>\n",
       "      <th>tfidf_scores</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment_rate</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0969717556831851, 0.0992437521512091, 0.263...</td>\n",
       "      <td>[2.799529422488119, 5.257354190516566, 8.06054...</td>\n",
       "      <td>i admit the great majority of films released b...</td>\n",
       "      <td>58.068170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1158965177773626, 0.08701826726418, 0.54519...</td>\n",
       "      <td>[3.2111204026718703, 4.457153013792461, 10.028...</td>\n",
       "      <td>take low_budget inexperienced_actors doubling_...</td>\n",
       "      <td>82.032456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0990853241104944, 0.1586218447658028, 0.112...</td>\n",
       "      <td>[5.330593352884969, 3.9433297836124703, 3.0233...</td>\n",
       "      <td>everybody has seen back to the future right wh...</td>\n",
       "      <td>145.001353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0764330961170044, 0.1491461881533719, 0.119...</td>\n",
       "      <td>[7.615105664819319, 2.8756240757651703, 1.9254...</td>\n",
       "      <td>doris_day was an icon of beauty in singing and...</td>\n",
       "      <td>61.576902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.1340982472710142, 0.1091749086227569, 0.22...</td>\n",
       "      <td>[5.166800755829627, 3.647351465867507, 4.19818...</td>\n",
       "      <td>after series of silly fun_loving movies was bi...</td>\n",
       "      <td>-8.479494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>[0.098878489634913, 0.1098077394959636, 0.1428...</td>\n",
       "      <td>[7.204677767369026, 4.026211480838702, 7.38906...</td>\n",
       "      <td>delightfully awful made by david_giancola guy ...</td>\n",
       "      <td>58.590931</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>[0.1124688754032146, 0.2298015385189905, 0.179...</td>\n",
       "      <td>[3.113804569902385, 9.334891634422286, 2.23652...</td>\n",
       "      <td>watching time_chasers it obvious that it was m...</td>\n",
       "      <td>39.725089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>[0.1351095157576171, 0.2637897870587913, 0.120...</td>\n",
       "      <td>[1.8396738171354543, 9.06811575801599, 3.98066...</td>\n",
       "      <td>at the beginning we can see members of troma_t...</td>\n",
       "      <td>150.601327</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>[0.2637897870587913, 0.1630778173203641, 0.149...</td>\n",
       "      <td>[2.0151368351146646, 1.4918603236549126, 2.875...</td>\n",
       "      <td>the movie was incredible ever since saw it in ...</td>\n",
       "      <td>23.817659</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>[0.0683768580468625, 0.1164429999796609, 0.120...</td>\n",
       "      <td>[7.146969449748379, 4.192740867191352, 3.17526...</td>\n",
       "      <td>tcm came through by acquiring this wonderful s...</td>\n",
       "      <td>49.612482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentiment_coeff  \\\n",
       "0      [0.0969717556831851, 0.0992437521512091, 0.263...   \n",
       "1      [0.1158965177773626, 0.08701826726418, 0.54519...   \n",
       "2      [0.0990853241104944, 0.1586218447658028, 0.112...   \n",
       "3      [0.0764330961170044, 0.1491461881533719, 0.119...   \n",
       "4      [-0.1340982472710142, 0.1091749086227569, 0.22...   \n",
       "...                                                  ...   \n",
       "49995  [0.098878489634913, 0.1098077394959636, 0.1428...   \n",
       "49996  [0.1124688754032146, 0.2298015385189905, 0.179...   \n",
       "49997  [0.1351095157576171, 0.2637897870587913, 0.120...   \n",
       "49998  [0.2637897870587913, 0.1630778173203641, 0.149...   \n",
       "49999  [0.0683768580468625, 0.1164429999796609, 0.120...   \n",
       "\n",
       "                                            tfidf_scores  \\\n",
       "0      [2.799529422488119, 5.257354190516566, 8.06054...   \n",
       "1      [3.2111204026718703, 4.457153013792461, 10.028...   \n",
       "2      [5.330593352884969, 3.9433297836124703, 3.0233...   \n",
       "3      [7.615105664819319, 2.8756240757651703, 1.9254...   \n",
       "4      [5.166800755829627, 3.647351465867507, 4.19818...   \n",
       "...                                                  ...   \n",
       "49995  [7.204677767369026, 4.026211480838702, 7.38906...   \n",
       "49996  [3.113804569902385, 9.334891634422286, 2.23652...   \n",
       "49997  [1.8396738171354543, 9.06811575801599, 3.98066...   \n",
       "49998  [2.0151368351146646, 1.4918603236549126, 2.875...   \n",
       "49999  [7.146969449748379, 4.192740867191352, 3.17526...   \n",
       "\n",
       "                                                  review  sentiment_rate  \\\n",
       "0      i admit the great majority of films released b...       58.068170   \n",
       "1      take low_budget inexperienced_actors doubling_...       82.032456   \n",
       "2      everybody has seen back to the future right wh...      145.001353   \n",
       "3      doris_day was an icon of beauty in singing and...       61.576902   \n",
       "4      after series of silly fun_loving movies was bi...       -8.479494   \n",
       "...                                                  ...             ...   \n",
       "49995  delightfully awful made by david_giancola guy ...       58.590931   \n",
       "49996  watching time_chasers it obvious that it was m...       39.725089   \n",
       "49997  at the beginning we can see members of troma_t...      150.601327   \n",
       "49998  the movie was incredible ever since saw it in ...       23.817659   \n",
       "49999  tcm came through by acquiring this wonderful s...       49.612482   \n",
       "\n",
       "       prediction  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               0  \n",
       "...           ...  \n",
       "49995           1  \n",
       "49996           1  \n",
       "49997           1  \n",
       "49998           1  \n",
       "49999           1  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf31f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
