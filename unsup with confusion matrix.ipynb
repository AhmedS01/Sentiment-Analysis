{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32f2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "from re import sub\n",
    "from time import time \n",
    "from unidecode import unidecode\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "import glob\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739fb079",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_neg=r'C:\\Users\\raja4\\Downloads\\aclImdb_v1.tar\\aclImdb\\test\\neg'\n",
    "path_test_pos=r'C:\\Users\\raja4\\Downloads\\aclImdb_v1.tar\\aclImdb\\test\\pos'\n",
    "file_list_neg_test = glob.glob(path_test_neg + \"/*.txt\")\n",
    "file_list_pos_test = glob.glob(path_test_pos + \"/*.txt\")\n",
    "test_neg_reviews=[]\n",
    "for i in range(0,len(file_list_neg_test)):\n",
    "    data=pd.read_table(file_list_neg_test[i])\n",
    "    test_neg_reviews.append(data.columns.tolist()[0])\n",
    "test_pos_reviews=[]\n",
    "for i in range(0,len(file_list_pos_test)):\n",
    "    data=pd.read_table(file_list_pos_test[i])\n",
    "    test_pos_reviews.append(data.columns.tolist()[0])\n",
    "\n",
    "test_x=test_pos_reviews+test_neg_reviews\n",
    "test_y=np.append(np.ones((len(test_pos_reviews))), np.zeros((len(test_neg_reviews))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d811df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(list(zip(test_y, test_x)), columns =['test_y', 'test_x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "570f9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d4c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    \"\"\"\n",
    "    Preprocess and convert texts to a list of words \n",
    "    \n",
    "    \"\"\"\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove html tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Single character removal\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    \n",
    "    text = text.split()\n",
    "    clean_text=[]\n",
    "    for word in text:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "              # stemming word\n",
    "            clean_text.append(word)\n",
    "        else:\n",
    "            clean_text.append(\" \")\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34cb1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['test_x']=df_test['test_x'].apply(lambda x: text_to_word_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92e047b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [ , went,  , saw,  , movie, last, night,  ,  ,...\n",
       "1        [actor, turned, director, bill, paxton, follow...\n",
       "2        [ , recreational, golfer,  ,  , knowledge,  , ...\n",
       "3        [ , saw,  , film,  , sneak, preview,  ,  ,  , ...\n",
       "4        [bill, paxton,  , taken,  , true, story,  ,  ,...\n",
       "                               ...                        \n",
       "24995    [ , occasionally, let,  , kids, watch,  , garb...\n",
       "24996    [ ,  ,  ,  , anymore,  , pretty, much, reality...\n",
       "24997    [ , basic, genre,  , thriller, intercut,  ,  ,...\n",
       "24998    [four, things, intrigued,  ,  ,  ,  , film, fi...\n",
       "24999    [david, bryce, comments, nearby,  , exceptiona...\n",
       "Name: test_x, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['test_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa25aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c8e3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'went',\n",
       " ' ',\n",
       " 'saw',\n",
       " ' ',\n",
       " 'movie',\n",
       " 'last_night',\n",
       " ' ',\n",
       " ' ',\n",
       " 'coaxed',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'friends',\n",
       " ' ',\n",
       " 'mine',\n",
       " ' ',\n",
       " 'admit',\n",
       " ' ',\n",
       " ' ',\n",
       " 'reluctant',\n",
       " ' ',\n",
       " 'see',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'knew',\n",
       " ' ',\n",
       " 'ashton_kutcher',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'able',\n",
       " ' ',\n",
       " ' ',\n",
       " 'comedy',\n",
       " ' ',\n",
       " 'wrong',\n",
       " 'kutcher',\n",
       " 'played',\n",
       " ' ',\n",
       " 'character',\n",
       " ' ',\n",
       " 'jake_fischer',\n",
       " ' ',\n",
       " 'well',\n",
       " ' ',\n",
       " 'kevin_costner',\n",
       " 'played',\n",
       " 'ben_randall',\n",
       " ' ',\n",
       " ' ',\n",
       " 'professionalism',\n",
       " ' ',\n",
       " 'sign',\n",
       " ' ',\n",
       " 'good',\n",
       " 'movie',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'toy',\n",
       " ' ',\n",
       " ' ',\n",
       " 'emotions',\n",
       " ' ',\n",
       " 'one',\n",
       " ' ',\n",
       " 'exactly',\n",
       " ' ',\n",
       " ' ',\n",
       " 'entire',\n",
       " 'theater',\n",
       " ' ',\n",
       " ' ',\n",
       " 'sold',\n",
       " ' ',\n",
       " ' ',\n",
       " 'overcome',\n",
       " ' ',\n",
       " 'laughter',\n",
       " ' ',\n",
       " ' ',\n",
       " 'first_half',\n",
       " ' ',\n",
       " ' ',\n",
       " 'movie',\n",
       " ' ',\n",
       " ' ',\n",
       " 'moved',\n",
       " ' ',\n",
       " 'tears',\n",
       " ' ',\n",
       " ' ',\n",
       " 'second_half',\n",
       " ' ',\n",
       " 'exiting',\n",
       " ' ',\n",
       " 'theater',\n",
       " ' ',\n",
       " ' ',\n",
       " 'saw',\n",
       " 'many',\n",
       " 'women',\n",
       " ' ',\n",
       " 'tears',\n",
       " ' ',\n",
       " 'many',\n",
       " 'full_grown',\n",
       " 'men',\n",
       " ' ',\n",
       " 'well',\n",
       " 'trying_desperately',\n",
       " ' ',\n",
       " ' ',\n",
       " 'let',\n",
       " 'anyone',\n",
       " 'see',\n",
       " ' ',\n",
       " 'crying',\n",
       " ' ',\n",
       " 'movie',\n",
       " ' ',\n",
       " 'great',\n",
       " ' ',\n",
       " 'suggest',\n",
       " ' ',\n",
       " ' ',\n",
       " 'go',\n",
       " 'see',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'judge']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = [row for row in df_test['test_x']]\n",
    "phrases_test = Phrases(text_test, min_count=1)\n",
    "bigram_test = Phraser(phrases_test)\n",
    "sentences_test = bigram_test[text_test]\n",
    "sentences_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bf6f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab for test: 0.11 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model_test = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     vector_size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model_test.build_vocab(sentences_test, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab for test: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd999f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model test: 3.39 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model_test.train(sentences_test, total_examples=w2v_model_test.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model test: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02afc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_test.save(\"word2vec_test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d07f3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export_test = df_test.copy()\n",
    "file_export_test['test_x'] = file_export_test['test_x'].apply(lambda x: ' '.join(bigram_test[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "223f8fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  went   saw   movie last_night     coaxed       friends   mine   admit     reluctant   see         knew   ashton_kutcher       able     comedy   wrong kutcher played   character   jake_fischer   well   kevin_costner played ben_randall     professionalism   sign   good movie         toy     emotions   one   exactly     entire theater     sold     overcome   laughter     first_half     movie     moved   tears     second_half   exiting   theater     saw many women   tears   many full_grown men   well trying_desperately     let anyone see   crying   movie   great   suggest     go see       judge'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_export_test['test_x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2cdc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eafabd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export_test['test_x'].to_csv(\"test_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1fe6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_test=w2v_model_test.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a280025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04780941,  0.14622356,  0.14958371, ...,  0.25098464,\n",
       "        -0.0602538 , -0.0591798 ],\n",
       "       [ 0.3445924 ,  0.4375354 ,  0.0115928 , ...,  0.5551704 ,\n",
       "         0.11971668, -0.22544034],\n",
       "       [ 0.37882426,  0.02022425,  0.20055208, ...,  0.02144994,\n",
       "         0.22550687, -0.39498693],\n",
       "       ...,\n",
       "       [-0.01057147, -0.06345598, -0.04084519, ..., -0.14383759,\n",
       "        -0.0158386 , -0.02100298],\n",
       "       [ 0.13612935, -0.0529405 , -0.07582344, ..., -0.1239469 ,\n",
       "        -0.08291385,  0.05322327],\n",
       "       [ 0.05790737, -0.11217798,  0.0027252 , ..., -0.04053995,\n",
       "        -0.04258352, -0.02639282]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_test.vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efe259a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca488a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 10)\n",
    "word_vectors_test.vectors = pca.fit_transform(word_vectors_test.vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acf557bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9934438 ,  0.6696675 , -0.81136155, -0.20199941,  0.88867635,\n",
       "        0.01777962,  0.44718662, -0.1768033 ,  0.5404171 , -0.31571737],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "074781af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test=KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors_test.vectors.astype('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69a63b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.07004111e-01, -3.19967674e-02, -1.89575399e-02,\n",
       "         2.99364983e-02,  2.10783659e-02,  8.20303736e-04,\n",
       "         1.17997111e-02, -3.11230764e-04,  1.10617553e-03,\n",
       "        -1.25227332e-03],\n",
       "       [ 8.66757626e-01,  3.43647084e-02,  2.03611499e-02,\n",
       "        -3.21532850e-02, -2.26401349e-02, -8.80605365e-04,\n",
       "        -1.26738971e-02,  3.33946664e-04, -1.18784607e-03,\n",
       "         1.34578841e-03]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ee91df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unknowingly', 0.9851993322372437),\n",
       " ('riccardo', 0.9774933457374573),\n",
       " ('gathers', 0.9709517359733582),\n",
       " ('yuk_su', 0.9683074355125427),\n",
       " ('accidentally_killed', 0.9680965542793274),\n",
       " ('momma_boy', 0.9644542932510376),\n",
       " ('lederer', 0.9642254710197449),\n",
       " ('goo_goo', 0.9638940691947937),\n",
       " ('willoughby', 0.9626596570014954),\n",
       " ('recently_deceased', 0.9602170586585999),\n",
       " ('pinkerton', 0.9592806696891785),\n",
       " ('braves', 0.9587650299072266),\n",
       " ('trusts', 0.9582815170288086),\n",
       " ('retires', 0.9581400752067566),\n",
       " ('master_yat', 0.9572418332099915),\n",
       " ('illegal_immigrant', 0.9570226073265076),\n",
       " ('siu_yu', 0.9566438794136047),\n",
       " ('john_eldredge', 0.9562990665435791),\n",
       " ('evee', 0.9557904005050659),\n",
       " ('rancher', 0.9545687437057495),\n",
       " ('former_girlfriend', 0.9543712735176086),\n",
       " ('tow', 0.9542824029922485),\n",
       " ('evelyn_keyes', 0.9529438614845276),\n",
       " ('young_orphan', 0.9506043195724487),\n",
       " ('mcbee', 0.9505482316017151),\n",
       " ('flees', 0.9503291845321655),\n",
       " ('kensuke', 0.9500924348831177),\n",
       " ('juanita', 0.9499669671058655),\n",
       " ('tenant', 0.9498148560523987),\n",
       " ('soon_discovers', 0.9497111439704895),\n",
       " ('brewing', 0.9482129812240601),\n",
       " ('harried', 0.9476897716522217),\n",
       " ('louis_bernard', 0.9456467628479004),\n",
       " ('getting_caught', 0.9456093311309814),\n",
       " ('assef', 0.9449969530105591),\n",
       " ('prof', 0.9447806477546692),\n",
       " ('criminal_element', 0.9446755051612854),\n",
       " ('fallon', 0.9446282982826233),\n",
       " ('arvidson', 0.9445458650588989),\n",
       " ('hugo', 0.9435076117515564),\n",
       " ('unbeknownst', 0.9431871175765991),\n",
       " ('billboard_model', 0.9425750970840454),\n",
       " ('kidnaps', 0.9421384930610657),\n",
       " ('former_cop', 0.9409492611885071),\n",
       " ('camden', 0.9408861994743347),\n",
       " ('landowner', 0.940599262714386),\n",
       " ('persuades', 0.9405552744865417),\n",
       " ('chin_chin', 0.9403243660926819),\n",
       " ('schuman', 0.9402097463607788),\n",
       " ('vagrant', 0.9393357038497925),\n",
       " ('offing', 0.9392064213752747),\n",
       " ('dinner_party', 0.9391804337501526),\n",
       " ('prostitution_ring', 0.9383845329284668),\n",
       " ('kindly', 0.9382932782173157),\n",
       " ('robberies', 0.9382886290550232),\n",
       " ('hatcher', 0.9373944401741028),\n",
       " ('local_law', 0.9372570514678955),\n",
       " ('mountain_lodge', 0.937175452709198),\n",
       " ('enforcement', 0.9370813965797424),\n",
       " ('faked_death', 0.9370429515838623),\n",
       " ('complications_arise', 0.9369520545005798),\n",
       " ('goes_berserk', 0.936277449131012),\n",
       " ('edelmann', 0.9361600279808044),\n",
       " ('enlists', 0.9358450770378113),\n",
       " ('kilt', 0.9352787137031555),\n",
       " ('jean_marie', 0.935170590877533),\n",
       " ('pregnant_wife', 0.9349195957183838),\n",
       " ('ridley', 0.9348059296607971),\n",
       " ('summons', 0.9347193837165833),\n",
       " ('benning', 0.9343571066856384),\n",
       " ('arranges', 0.9341060519218445),\n",
       " ('white_bull', 0.9332958459854126),\n",
       " ('leaded', 0.9331748485565186),\n",
       " ('johanna_dog', 0.9329543113708496),\n",
       " ('louden', 0.932941734790802),\n",
       " ('accidentally_shoots', 0.9328669905662537),\n",
       " ('cahoots', 0.9328009486198425),\n",
       " ('savings', 0.9324851632118225),\n",
       " ('schwiefka', 0.9324040412902832),\n",
       " ('bar_tender', 0.9323176741600037),\n",
       " ('upton', 0.9321370720863342),\n",
       " ('wellington', 0.9320958256721497),\n",
       " ('nevada', 0.9319494366645813),\n",
       " ('bahunda', 0.9318439960479736),\n",
       " ('intercedes', 0.9316346049308777),\n",
       " ('scientist_joseph', 0.9315977096557617),\n",
       " ('kay_francis', 0.9313717484474182),\n",
       " ('blythe', 0.9309526085853577),\n",
       " ('gaby', 0.9309367537498474),\n",
       " ('patricia_martin', 0.9304801821708679),\n",
       " ('laborer', 0.9302886724472046),\n",
       " ('deposited', 0.9299208521842957),\n",
       " ('seriously_injured', 0.929868221282959),\n",
       " ('open_seafood', 0.9297971129417419),\n",
       " ('red_cross', 0.9297176003456116),\n",
       " ('katona', 0.9294775724411011),\n",
       " ('realtor', 0.9294285178184509),\n",
       " ('midland_lee', 0.929313600063324),\n",
       " ('deceased', 0.9285569190979004),\n",
       " ('kent_taylor', 0.9282945990562439)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_test.similar_by_vector(model_test.cluster_centers_[1], topn=100, restrict_vocab=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0c1864c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worse', 0.9694814085960388),\n",
       " ('sucks', 0.9684250354766846),\n",
       " ('horrible', 0.9671058654785156),\n",
       " ('terrible', 0.9616751670837402),\n",
       " ('even_worse', 0.9559447765350342),\n",
       " ('awful', 0.950816810131073),\n",
       " ('coherent_script', 0.9444460868835449),\n",
       " ('normal_scale', 0.939327597618103),\n",
       " ('minutes_tops', 0.9386139512062073),\n",
       " ('lame', 0.9349110126495361),\n",
       " ('ever_seen', 0.9316216707229614),\n",
       " ('even', 0.931445300579071),\n",
       " ('product_placement', 0.930504560470581),\n",
       " ('tornado_tommy', 0.9297540783882141),\n",
       " ('funny_bits', 0.9290056228637695)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_test.most_similar('bad',topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91782479",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_center = model_test.cluster_centers_[0]\n",
    "negative_cluster_center = model_test.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "417ec96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test = pd.DataFrame(word_vectors_test.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0474a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test.columns = ['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf06ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test['vectors'] = words_test.words.apply(lambda x: word_vectors_test[f'{x}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50a10233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_test['vectors'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9db3602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 41.63 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "words_test[\"cluster\"] = words_test[\"vectors\"].apply(lambda x: model_test.predict([x.astype('double')]))\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b588adbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>[-0.9934438, 0.6696675, -0.81136155, -0.201999...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie</td>\n",
       "      <td>[-2.8646297, 1.4718636, -0.92826766, 0.0476076...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film</td>\n",
       "      <td>[-2.8255777, -0.80261135, -0.5002815, -0.12243...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one</td>\n",
       "      <td>[-1.2143981, 1.0122045, -0.4404688, -0.0133457...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>[-1.4653059, 1.7439641, -0.30428168, -0.303772...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54646</th>\n",
       "      <td>cha</td>\n",
       "      <td>[-0.59306616, 0.33769694, -0.09070069, 0.05452...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54647</th>\n",
       "      <td>danged</td>\n",
       "      <td>[0.088530414, 0.21273738, 0.2350936, 0.0089622...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54648</th>\n",
       "      <td>corset</td>\n",
       "      <td>[0.15470609, 0.17892982, -0.19603989, -0.05230...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54649</th>\n",
       "      <td>neutralizing</td>\n",
       "      <td>[-0.008716064, -0.16565132, -0.7028671, -0.176...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54650</th>\n",
       "      <td>record_player</td>\n",
       "      <td>[0.38298514, -0.032597136, -0.2652347, 0.12494...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54651 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               words                                            vectors  \\\n",
       "0                     [-0.9934438, 0.6696675, -0.81136155, -0.201999...   \n",
       "1              movie  [-2.8646297, 1.4718636, -0.92826766, 0.0476076...   \n",
       "2               film  [-2.8255777, -0.80261135, -0.5002815, -0.12243...   \n",
       "3                one  [-1.2143981, 1.0122045, -0.4404688, -0.0133457...   \n",
       "4               like  [-1.4653059, 1.7439641, -0.30428168, -0.303772...   \n",
       "...              ...                                                ...   \n",
       "54646            cha  [-0.59306616, 0.33769694, -0.09070069, 0.05452...   \n",
       "54647         danged  [0.088530414, 0.21273738, 0.2350936, 0.0089622...   \n",
       "54648         corset  [0.15470609, 0.17892982, -0.19603989, -0.05230...   \n",
       "54649   neutralizing  [-0.008716064, -0.16565132, -0.7028671, -0.176...   \n",
       "54650  record_player  [0.38298514, -0.032597136, -0.2652347, 0.12494...   \n",
       "\n",
       "      cluster  \n",
       "0         [0]  \n",
       "1         [0]  \n",
       "2         [0]  \n",
       "3         [0]  \n",
       "4         [0]  \n",
       "...       ...  \n",
       "54646     [0]  \n",
       "54647     [1]  \n",
       "54648     [1]  \n",
       "54649     [0]  \n",
       "54650     [1]  \n",
       "\n",
       "[54651 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e7d443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test.cluster = words_test.cluster.apply(lambda x: x[0])\n",
    "words_test['cluster_value'] = [1 if i==0 else -1 for i in words_test.cluster]\n",
    "words_test['closeness_score'] = words_test.apply(lambda x: 1/(model_test.transform([x.vectors]).min()), axis=1)\n",
    "words_test['sentiment_coeff'] = words_test.closeness_score * words_test.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72083006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>[-0.9934438, 0.6696675, -0.81136155, -0.201999...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623509</td>\n",
       "      <td>0.623509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie</td>\n",
       "      <td>[-2.8646297, 1.4718636, -0.92826766, 0.0476076...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.283096</td>\n",
       "      <td>0.283096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film</td>\n",
       "      <td>[-2.8255777, -0.80261135, -0.5002815, -0.12243...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.378917</td>\n",
       "      <td>0.378917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one</td>\n",
       "      <td>[-1.2143981, 1.0122045, -0.4404688, -0.0133457...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602923</td>\n",
       "      <td>0.602923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>[-1.4653059, 1.7439641, -0.30428168, -0.303772...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.419065</td>\n",
       "      <td>0.419065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good</td>\n",
       "      <td>[-2.0575356, 0.94678575, 0.9828352, -0.4168552...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332057</td>\n",
       "      <td>0.332057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>would</td>\n",
       "      <td>[-1.7099445, 1.0980903, -1.3524666, 1.049664, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.361347</td>\n",
       "      <td>0.361347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>time</td>\n",
       "      <td>[-1.327281, 1.0317596, -0.9380007, 0.57386595,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.458329</td>\n",
       "      <td>0.458329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>really</td>\n",
       "      <td>[-2.0235353, 1.2417774, -0.46951735, -1.190692...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.301810</td>\n",
       "      <td>0.301810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>see</td>\n",
       "      <td>[-1.5053174, 1.3558956, -1.078633, 0.7393307, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.380213</td>\n",
       "      <td>0.380213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words                                            vectors  cluster  \\\n",
       "0          [-0.9934438, 0.6696675, -0.81136155, -0.201999...        0   \n",
       "1   movie  [-2.8646297, 1.4718636, -0.92826766, 0.0476076...        0   \n",
       "2    film  [-2.8255777, -0.80261135, -0.5002815, -0.12243...        0   \n",
       "3     one  [-1.2143981, 1.0122045, -0.4404688, -0.0133457...        0   \n",
       "4    like  [-1.4653059, 1.7439641, -0.30428168, -0.303772...        0   \n",
       "5    good  [-2.0575356, 0.94678575, 0.9828352, -0.4168552...        0   \n",
       "6   would  [-1.7099445, 1.0980903, -1.3524666, 1.049664, ...        0   \n",
       "7    time  [-1.327281, 1.0317596, -0.9380007, 0.57386595,...        0   \n",
       "8  really  [-2.0235353, 1.2417774, -0.46951735, -1.190692...        0   \n",
       "9     see  [-1.5053174, 1.3558956, -1.078633, 0.7393307, ...        0   \n",
       "\n",
       "   cluster_value  closeness_score  sentiment_coeff  \n",
       "0              1         0.623509         0.623509  \n",
       "1              1         0.283096         0.283096  \n",
       "2              1         0.378917         0.378917  \n",
       "3              1         0.602923         0.602923  \n",
       "4              1         0.419065         0.419065  \n",
       "5              1         0.332057         0.332057  \n",
       "6              1         0.361347         0.361347  \n",
       "7              1         0.458329         0.458329  \n",
       "8              1         0.301810         0.301810  \n",
       "9              1         0.380213         0.380213  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "389ba7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5af8fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66e1f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2d870ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map_test = pd.read_csv(\"sentiment_dictionary_test.csv\")\n",
    "sentiment_dict_test = dict(zip(sentiment_map_test[\"words\"].values, sentiment_map_test[\"sentiment_coeff\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40d6c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fbb42fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raja4\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\raja4\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tfidf_test = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf_test.fit(test['test_x'].values.astype('U'))\n",
    "features_test = pd.Series(tfidf_test.get_feature_names())\n",
    "review_tfidf_test = tfidf_test.transform(test['test_x'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a80d8619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    '''\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "    \n",
    "    inspired  by function from this article: \n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    \n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    '''\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x['test_x'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fea26ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          went   saw   movie last_night     coaxed    ...\n",
       "1        actor turned director bill_paxton follows     ...\n",
       "2          recreational golfer     knowledge     sport ...\n",
       "3          saw   film   sneak_preview       delightful ...\n",
       "4        bill_paxton   taken   true story     us golf o...\n",
       "                               ...                        \n",
       "24995      occasionally let   kids watch   garbage     ...\n",
       "24996            anymore   pretty_much reality tv_shows...\n",
       "24997      basic genre   thriller intercut     uncomfor...\n",
       "24998    four things intrigued         film firstly   s...\n",
       "24999    david bryce comments nearby   exceptionally_we...\n",
       "Name: test_x, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['test_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4fc6d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.7 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "replaced_tfidf_scores_test = test.apply(lambda x: replace_tfidf_words(x, review_tfidf_test, features_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a61e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    \"\"\"\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad178fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_closeness_scores_test = test['test_x'].apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict_test), str(x).split())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "835e8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df_test = pd.DataFrame(data=[replaced_closeness_scores_test, replaced_tfidf_scores_test, test['test_x']]).T\n",
    "replacement_df_test.columns = [\"sentiment_coeff\", \"tfidf_scores\", \"review\"]\n",
    "replacement_df_test[\"sentiment_rate\"] = replacement_df_test.apply(lambda x: np.array(x.loc[\"sentiment_coeff\"]) @ np.array(x.loc[\"tfidf_scores\"]), axis=1)\n",
    "replacement_df_test['prediction'] = (replacement_df_test.sentiment_rate>0).astype('int8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83173d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_coeff</th>\n",
       "      <th>tfidf_scores</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment_rate</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.2437598677548857, 0.1938759915768454, 0.283...</td>\n",
       "      <td>[3.9337368818345597, 6.4654519296287996, 5.986...</td>\n",
       "      <td>went   saw   movie last_night     coaxed    ...</td>\n",
       "      <td>40.662468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.191906983386557, 0.4077534436707549, 0.2678...</td>\n",
       "      <td>[3.69418729513324, 4.379084576221043, 3.104102...</td>\n",
       "      <td>actor turned director bill_paxton follows     ...</td>\n",
       "      <td>115.988909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.9953220646976476, -0.6556286576736207, 0.2...</td>\n",
       "      <td>[9.740376741930469, 9.334911633822305, 5.65861...</td>\n",
       "      <td>recreational golfer     knowledge     sport ...</td>\n",
       "      <td>54.069739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.1938759915768454, 0.3789167259852144, 0.251...</td>\n",
       "      <td>[3.2327259648143998, 4.810478216223833, 7.9077...</td>\n",
       "      <td>saw   film   sneak_preview       delightful ...</td>\n",
       "      <td>74.919774</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.2942555339698585, -0.4046634064908467, 0.31...</td>\n",
       "      <td>[7.660935200250632, 4.360479388390009, 3.65530...</td>\n",
       "      <td>bill_paxton   taken   true story     us golf o...</td>\n",
       "      <td>99.515839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>[0.2467135571075348, 0.3197506561352115, -0.16...</td>\n",
       "      <td>[5.589336836031823, 3.6289094024277904, 4.0233...</td>\n",
       "      <td>occasionally let   kids watch   garbage     ...</td>\n",
       "      <td>35.008194</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>[0.240237924442717, 0.2989973164576753, 0.2505...</td>\n",
       "      <td>[5.3245527276732965, 4.910065001965494, 4.4370...</td>\n",
       "      <td>anymore   pretty_much reality tv_shows...</td>\n",
       "      <td>106.203477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>[0.2116394030408076, 0.1953313356150229, 0.180...</td>\n",
       "      <td>[5.554517070872595, 4.339954152445278, 9.56558...</td>\n",
       "      <td>basic genre   thriller intercut     uncomfor...</td>\n",
       "      <td>182.540938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>[0.3013105718953794, 0.2999432151159756, 0.238...</td>\n",
       "      <td>[4.746548566150594, 3.1621677394988112, 6.5940...</td>\n",
       "      <td>four things intrigued         film firstly   s...</td>\n",
       "      <td>75.247665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>[-0.21691366870739, -0.988264078077036, 0.2146...</td>\n",
       "      <td>[5.650207551118848, 9.517233190616258, 4.80052...</td>\n",
       "      <td>david bryce comments nearby   exceptionally_we...</td>\n",
       "      <td>146.906064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentiment_coeff  \\\n",
       "0      [0.2437598677548857, 0.1938759915768454, 0.283...   \n",
       "1      [0.191906983386557, 0.4077534436707549, 0.2678...   \n",
       "2      [-0.9953220646976476, -0.6556286576736207, 0.2...   \n",
       "3      [0.1938759915768454, 0.3789167259852144, 0.251...   \n",
       "4      [0.2942555339698585, -0.4046634064908467, 0.31...   \n",
       "...                                                  ...   \n",
       "24995  [0.2467135571075348, 0.3197506561352115, -0.16...   \n",
       "24996  [0.240237924442717, 0.2989973164576753, 0.2505...   \n",
       "24997  [0.2116394030408076, 0.1953313356150229, 0.180...   \n",
       "24998  [0.3013105718953794, 0.2999432151159756, 0.238...   \n",
       "24999  [-0.21691366870739, -0.988264078077036, 0.2146...   \n",
       "\n",
       "                                            tfidf_scores  \\\n",
       "0      [3.9337368818345597, 6.4654519296287996, 5.986...   \n",
       "1      [3.69418729513324, 4.379084576221043, 3.104102...   \n",
       "2      [9.740376741930469, 9.334911633822305, 5.65861...   \n",
       "3      [3.2327259648143998, 4.810478216223833, 7.9077...   \n",
       "4      [7.660935200250632, 4.360479388390009, 3.65530...   \n",
       "...                                                  ...   \n",
       "24995  [5.589336836031823, 3.6289094024277904, 4.0233...   \n",
       "24996  [5.3245527276732965, 4.910065001965494, 4.4370...   \n",
       "24997  [5.554517070872595, 4.339954152445278, 9.56558...   \n",
       "24998  [4.746548566150594, 3.1621677394988112, 6.5940...   \n",
       "24999  [5.650207551118848, 9.517233190616258, 4.80052...   \n",
       "\n",
       "                                                  review  sentiment_rate  \\\n",
       "0        went   saw   movie last_night     coaxed    ...       40.662468   \n",
       "1      actor turned director bill_paxton follows     ...      115.988909   \n",
       "2        recreational golfer     knowledge     sport ...       54.069739   \n",
       "3        saw   film   sneak_preview       delightful ...       74.919774   \n",
       "4      bill_paxton   taken   true story     us golf o...       99.515839   \n",
       "...                                                  ...             ...   \n",
       "24995    occasionally let   kids watch   garbage     ...       35.008194   \n",
       "24996          anymore   pretty_much reality tv_shows...      106.203477   \n",
       "24997    basic genre   thriller intercut     uncomfor...      182.540938   \n",
       "24998  four things intrigued         film firstly   s...       75.247665   \n",
       "24999  david bryce comments nearby   exceptionally_we...      146.906064   \n",
       "\n",
       "       prediction  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "24995           1  \n",
       "24996           1  \n",
       "24997           1  \n",
       "24998           1  \n",
       "24999           1  \n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1419111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1828</td>\n",
       "      <td>10672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2332</td>\n",
       "      <td>10168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1\n",
       "0  1828  10672\n",
       "1  2332  10168"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.479840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.487908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.813440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.609958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             scores\n",
       "accuracy   0.479840\n",
       "precision  0.487908\n",
       "recall     0.813440\n",
       "f1         0.609958"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_classes = replacement_df_test.prediction\n",
    "y_test = test_y\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(df_test['test_y'], replacement_df_test.prediction))\n",
    "print('Confusion Matrix')\n",
    "display(conf_matrix)\n",
    "\n",
    "test_scores = accuracy_score(y_test,predicted_classes), precision_score(y_test, predicted_classes), recall_score(y_test, predicted_classes), f1_score(y_test, predicted_classes)\n",
    "\n",
    "print('\\n \\n Scores')\n",
    "scores = pd.DataFrame(data=[test_scores])\n",
    "scores.columns = ['accuracy', 'precision', 'recall', 'f1']\n",
    "scores = scores.T\n",
    "scores.columns = ['scores']\n",
    "display(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c8c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
